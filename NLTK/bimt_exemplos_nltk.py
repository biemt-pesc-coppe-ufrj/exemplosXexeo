# -*- coding: utf-8 -*-
"""BIMT exemplos nltk.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t1wpi9UNI2fieQRPCGu2itTlNX9fD21k
"""

import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('words')
nltk.download('gutenberg')
nltk.download('genesis')
nltk.download('inaugural')
nltk.download('nps_chat')
nltk.download('webtext')
nltk.download('treebank')
nltk.download('udhr')
from nltk.book import *

"""# Tokenizando sentenÃ§as"""

from nltk.tokenize import sent_tokenize 

texto= "Once upon a time, there was a little girl who lived \
in a village near the forest.  Whenever she went out, the little girl \
wore a red riding cloak, so everyone in the \
village called her Little Red Riding Hood. \
One morning, Little Red Riding Hood asked her mother if she could go \
to visit her grandmother as it had been awhile since they'd \
seen each other. ""That's a good idea,"" her mother said.  So they packed \
a nice basket for Little Red Riding Hood to take to her grandmother." 

sentences = sent_tokenize(texto)
print(sentences)

"""# Tokenizando palavras"""

from nltk.tokenize import word_tokenize
from nltk.tokenize import wordpunct_tokenize

words = word_tokenize(sentences[0])
wordsp = wordpunct_tokenize(sentences[3])
print(words)
print(wordsp)

"""# Pos Tagging"""

from nltk.tag import pos_tag

tagged = pos_tag(words)

print(tagged)

"""# Chunks and Named Entity Recognition"""

from nltk.chunk import ne_chunk

chunked = ne_chunk(tagged)

print(chunked)

"""# Named Entity Recognition"""

for item in nltk.sent_tokenize(texto) :
    print(nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(item))))

text1.concordance("Ishmael")

text1.similar("Ishmael")

text1.similar("before")

text4.dispersion_plot(["citizens", "democracy", "freedom", "duties", "America"])

inaugural.fileids()

[fileid[:4] for fileid in inaugural.fileids()]

cfd = nltk.ConditionalFreqDist(
          (target, fileid[:4])
           for fileid in inaugural.fileids()
           for w in inaugural.words(fileid)
           for target in ['america', 'citizen']
           if w.lower().startswith(target))

cfd.plot()

from nltk.corpus import udhr
languages = ['Chickasaw', 'English', 'German_Deutsch','Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']

cfd = nltk.ConditionalFreqDist(
           (lang, len(word))
           for lang in languages
           for word in udhr.words(lang + '-Latin1'))

cfd.plot(cumulative=True)

from nltk.stem.porter import PorterStemmer
x=PorterStemmer()
r = set() 
rr = set()
for s in sentences :
  for w in word_tokenize(s) :
    t = x.stem(w)
    r.add(t)
    rr.add(w)
print(r)
print(rr)

r.intersection(rr)
print(r)

print(rr.difference(rr))
print(r.difference(rr))
print(rr.difference(r))

from urllib.request import urlopen
url = "http://www.cos.ufrj.br/"
raw = urlopen(url).read()
print( type(raw))
print( len(raw))
print( raw[:75])

from nltk.stem.porter import PorterStemmer
from bs4 import BeautifulSoup

PS= PorterStemmer()

url = "http://www.cos.ufrj.br/"
html = urlopen(url).read()
#raw = nltk.clean_html(html) # nÃ£o funciona mais, manda usar Beautiful soup
#txt = raw.decode("iso-8859-1") # versÃ£o anterior do nltk
raw = BeautifulSoup(html).get_text()
tokens = nltk.word_tokenize(raw)
terms = map(PS.stem,tokens)

for i in terms :
   print (i)

"""# Testando POS Tagger"""

nltk.download('tagsets')

tag_used = set()
text = word_tokenize(texto)

for word, tag in tagged:
  tag_used.add(tag)
  print(f"{word:12} -> {tag:6}")

for  tagu in tag_used:
  nltk.help.upenn_tagset(tagu)